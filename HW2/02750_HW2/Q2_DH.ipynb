{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "optimum-tracy",
   "metadata": {},
   "source": [
    "# Question 2: DH algorithm (50 points)\n",
    "In this question we are going to implmeneted the DH algorithm according to this paper:https://icml.cc/Conferences/2008/papers/324.pdf, in which we try to predict protein localization sites in Eukaryotic cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-solomon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import warnings\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import choice\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "seed = 2021\n",
    "warnings. filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-affiliation",
   "metadata": {},
   "source": [
    "## 2.0 Data loading and hierarchical clustering\n",
    "DH algorithm is based on hierarchical clustering of the dataset, we will use the DH algorithm on this classification problem: [Protein Localization Prediction](https://archive.ics.uci.edu/ml/datasets/Yeast), the first step is to load the dataset and conduct a hierarchical clustring using the **Scipy** package. *This part has been implemented, read through the code to make sure you understand what is being done.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-survival",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(seed = 2021):\n",
    "    \"\"\" Loads \"Protein Localizataion Prediction\" data. Computes linkage from hierarchical clustering.\n",
    "    :returns X_train: data matrix 1200x8\n",
    "    :returns Y_train: true labels 1200x1\n",
    "    :returns X_test: data matrix 284x8\n",
    "    :returns Y_test: true labels 284x1\n",
    "    :returns T: 3 element tree\n",
    "        T[0] = linkage matrix from hierarchical clustering.  See https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html\n",
    "               for details. If you are unfamiliar with hierarchical clustering using scipy, the following is another helpful resource (We won't use dendrograms\n",
    "               here, but he gives a nice explanation of how to interpret the linkage matrix):\n",
    "               https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/ \n",
    "\n",
    "        T[1] = An array denoting the size of each subtree rooted at node i, where i indexes the array.  \n",
    "               ie. The number of all leaves in subtree rooted at node i (w_i in the paper).\n",
    "\n",
    "        T[2] = dict where keys are nodes and values are the node's parent\n",
    "        \"\"\"\n",
    "    df = pd.read_csv('./data/data.csv')\n",
    "    np.unique(df.Label,return_counts = True)\n",
    "    filter_class = ['MIT','NUC']\n",
    "    mask = df.Label ==0\n",
    "    for x in filter_class:\n",
    "        mask = mask | (df.Label==x)\n",
    "    df = df[mask]\n",
    "    X = df.iloc[:,:8].to_numpy()\n",
    "    y = df.Label.astype('category').cat.codes.to_numpy()\n",
    "    X, X_test, y, y_test = train_test_split(X,y,test_size = 0.2, random_state = seed)\n",
    "    n_samples = len(X)\n",
    "    Z = linkage(X,method='ward')\n",
    "    link = Z[:,:2].astype(int)\n",
    "    subtree_sizes = np.zeros(link[-1,-1]+2)\n",
    "    subtree_sizes[:n_samples] = 1\n",
    "    parent = {}\n",
    "    parent[2*(n_samples-1)] = 0 #set root node as 0\n",
    "    for i in range(len(link)):\n",
    "        left = link[i,0]\n",
    "        right = link[i,1]\n",
    "        current = i + n_samples\n",
    "        subtree_sizes[current] = subtree_sizes[left] + subtree_sizes[right] \n",
    "        parent[left] = current\n",
    "        parent[right] = current\n",
    "\n",
    "    T = [link,subtree_sizes,parent]\n",
    "\n",
    "    return X.astype(\"float\"), y, X_test, y_test, T \n",
    "\n",
    "X_train, y_train, X_test, y_test, T = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-bargain",
   "metadata": {},
   "source": [
    "# 2.0.1 Supervised classification methods.\n",
    "Following we provide several classifiers that can be used, choose your favourite one. To use the Neural Network classifier, you need to install [pytorch](https://pytorch.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-courtesy",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "## Random Forest\n",
    "N_estimator_rf = 20\n",
    "MAX_depth_rf = 6\n",
    "rf = RandomForestClassifier(n_estimators = N_estimator_rf, \n",
    "                            max_depth = MAX_depth_rf, random_state = seed)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "## Gradient Boosting Decision Tree\n",
    "N_estimator_gbdt = 20\n",
    "gbdt_max_depth = 6\n",
    "gbdt = GradientBoostingClassifier(n_estimators = N_estimator_gbdt,\n",
    "                                 learning_rate = 0.1,\n",
    "                                 max_depth = gbdt_max_depth,\n",
    "                                 random_state = seed)\n",
    "gbdt.fit(X_train,y_train)\n",
    "\n",
    "## 3-Layer fully connected NN\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "torch.manual_seed(seed)\n",
    "class NNClassifier(object):\n",
    "    def __init__(self,\n",
    "                 feature_n,\n",
    "                 class_n,\n",
    "                 hidden_n = 30,\n",
    "                 learning_rate = 4e-3,\n",
    "                 weight_decay = 1e-5):\n",
    "        self.model = torch.nn.Sequential(torch.nn.Linear(feature_n,hidden_n),\n",
    "                                        torch.nn.SiLU(),\n",
    "                                        torch.nn.Linear(hidden_n,hidden_n),\n",
    "                                        torch.nn.SiLU(),\n",
    "                                        torch.nn.Linear(hidden_n,class_n))\n",
    "        self.lr = learning_rate\n",
    "        self.wd = weight_decay\n",
    "    def fit(self,X_train,y_train,epoches = 300,batch_size = 50):\n",
    "        X_t = torch.from_numpy(X_train.astype(np.float32))\n",
    "        y_t = torch.from_numpy(y_train.astype(np.int64))\n",
    "        dataset = TensorDataset(X_t,y_t)\n",
    "        loader = DataLoader(dataset,batch_size = batch_size,shuffle = True)\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(reduction = 'mean')\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr,weight_decay=self.wd)\n",
    "        loss_record = 0.0\n",
    "        report_epoch = 50\n",
    "        for epoch_i in range(epoches):\n",
    "            for batch in loader:\n",
    "                x_batch,y_batch = batch\n",
    "                y_pred = self.model(x_batch)\n",
    "                loss = loss_fn(y_pred,y_batch)\n",
    "                self.model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_record += loss.item()\n",
    "            if epoch_i%report_epoch == report_epoch-1:\n",
    "                print(\"[%d|%d] epoch loss:%.2f\"%(epoch_i+1,epoches,loss_record/report_epoch))\n",
    "                loss_record = 0.0\n",
    "            if epoch_i>=epoches:\n",
    "                break\n",
    "    \n",
    "    def score(self,X_test,y_test):\n",
    "        X_test_tensor = torch.from_numpy(X_test.astype(np.float32))\n",
    "        y_pred_test = self.model(X_test_tensor)\n",
    "        y_output = torch.argmax(y_pred_test,axis = 1).numpy()\n",
    "        return (y_output == y_test).mean()\n",
    "        \n",
    "nn = NNClassifier(feature_n = X_train.shape[1],class_n = len(np.unique(y_train)))\n",
    "nn.fit(X_train,y_train)\n",
    "\n",
    "## Accuracy of 4 classifiers.\n",
    "print('Accuracy of logistic regression: \\t{:.3f}'.format(lr.score(X_test,y_test)))\n",
    "print('Accuracy of random forest: \\t\\t{:.3f}'.format(rf.score(X_test,y_test)))\n",
    "print('Accuracy of Gradient Boosting Decision Tree: \\t\\t{:.3f}'.format(gbdt.score(X_test,y_test)))\n",
    "print('Accuracy of Neural Network: \\t\\t{:.3f}'.format(nn.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-might",
   "metadata": {},
   "source": [
    "### Choose and initialize your classifier:\n",
    "The classifier is going to be used in 2.2, the choose of classifier won't influence your grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-reasoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Uncomment one line to choose your classifier.\n",
    "#classifier = LogisticRegression()\n",
    "\n",
    "#classifier = RandomForestClassifier(n_estimators = N_estimator_rf,max_depth = MAX_depth_rf, random_state = seed)\n",
    "\n",
    "#classifier = GradientBoostingClassifier(n_estimators = N_estimator_gbdt,\n",
    "#                                 learning_rate = 0.1,\n",
    "#                                 max_depth = gbdt_max_depth,\n",
    "#                                 random_state = seed)\n",
    "\n",
    "#classifier = NNClassifier(feature_n = X_train.shape[1],class_n = len(np.unique(y_train)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-moscow",
   "metadata": {},
   "source": [
    "## 2.1 Implement the DH algorithm (Hierarchical Sampling for Active Learning). (30 points)\n",
    "Please complete the functions to implement the DH algorithm and run the active learning algorithm on the training dataset. The utils functions has been implemented and attached in the homework folder, including **update_empirical.py, best_pruning_and_labeling.py, assign_labels.py and get_leaves.py**, please read them and finish the following functions to implement the DH algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-anderson",
   "metadata": {},
   "outputs": [],
   "source": [
    "from update_empirical import update_empirical\n",
    "from best_pruning_and_labeling import best_pruning_and_labeling\n",
    "from assign_labels import assign_labels\n",
    "from get_leaves import get_leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-algebra",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(L,labels):\n",
    "    \"\"\"Compute the error\n",
    "\n",
    "    :param L: labeling of leaf nodes\n",
    "    :param labels: true labels of each node\n",
    "\n",
    "    :returns error: error of predictions\"\"\"\n",
    "\n",
    "    wrong = 0\n",
    "    wrong = (L[:len(labels)]!=labels).sum()\n",
    "    error = wrong/len(labels)\n",
    "    return error\n",
    "\n",
    "def select_case_1(data,labels,T,budget,batch_size):\n",
    "    \"\"\"DH algorithm where we choose P proportional to the size of subtree rooted at each node\n",
    "\n",
    "    :param data: Data matrix 1200x8\n",
    "    :param labels: true labels 1200x1\n",
    "    :param T: 3 element tree\n",
    "        T[0] = linkage matrix from hierarchical clustering.  See https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html\n",
    "               for details. If you are unfamiliar with hierarchical clustering using scipy, the following is another helpful resource (We won't use dendrograms\n",
    "               here, but he gives a nice explanation of how to interpret the linkage matrix):\n",
    "               https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/ \n",
    "\n",
    "        T[1] = An array denoting the size of each subtree rooted at node i, where i indexes the array.  \n",
    "               ie. The number of all children + grandchildren + ... + the node itself\n",
    "\n",
    "        T[2] = dict where keys are nodes and values are the node's parent\n",
    "    :param budget: Number of iterations to make \n",
    "    :param batch_size: Number of queries per iteration\"\"\"\n",
    "\n",
    "    n_nodes = len(T[1]) #total nodes in T\n",
    "    n_samples = len(data) #total samples in data\n",
    "    L = np.zeros(n_nodes) #majority label\n",
    "    p1 = np.zeros(n_nodes) #empirical label frequency\n",
    "    n = np.zeros(n_nodes) #number of points sampled from each node\n",
    "    error = []#np.zeros(n_samples) #error at each round\n",
    "    root = n_nodes-1 #corresponds to index of root\n",
    "    P = np.array([root])\n",
    "    L[root] = 1    \n",
    "    \n",
    "    for i in range(budget):\n",
    "        selected_P = []\n",
    "        for b in range(batch_size):\n",
    "\n",
    "            #TODO: select a node from P proportional to the size of subtree rooted at each node\n",
    "            raise(NotImplemetedError)\n",
    "            \n",
    "            ##TODO: pick a random leaf node from subtree Tv and query its label\n",
    "\n",
    "            #TODO: update empirical counts and probabilities for all nodes u on path from z to v\n",
    "\n",
    "        for p in selected_P:\n",
    "            #TODO: update admissible A and compute scores; find best pruning and labeling\n",
    "            raise(NotImplemetedError)\n",
    "            #TODO: update pruning P and labeling L\n",
    "\n",
    "        #TODO: temporarily assign labels to every leaf and compute error\n",
    "        L_temp = L.copy()\n",
    "        raise(NotImplemetedError)\n",
    "\n",
    "    for i in range(len(P)):\n",
    "        L = assign_labels(L,P[i],P[i],T,n_samples)\n",
    "    \n",
    "    return L, np.array(error)\n",
    "\n",
    "def select_case_2(data,labels,T,budget,batch_size):\n",
    "    \"\"\"DH algorithm where we choose P by biasing towards choosing nodes in areas where the observed labels are less pure\n",
    "\n",
    "    :param data: Data matrix 1200x8\n",
    "    :param labels: true labels 284x1\n",
    "    :param T: 3 element tree\n",
    "        T[0] = linkage matrix from hierarchical clustering.  See https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html\n",
    "               for details. If you are unfamiliar with hierarchical clustering using scipy, the following is another helpful resource (We won't use dendrograms\n",
    "               here, but he gives a nice explanation of how to interpret the linkage matrix):\n",
    "               https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/ \n",
    "\n",
    "        T[1] = An array denoting the size of each subtree rooted at node i, where i indexes the array.  \n",
    "               ie. The number of all children + grandchildren + ... + the node itself\n",
    "\n",
    "        T[2] = dict where keys are nodes and values are the node's parent\n",
    "    :param budget: Number of iterations to make \n",
    "    :param batch_size: Number of queries per iteration\"\"\"\n",
    "\n",
    "    n_nodes = len(T[1]) #total nodes in T\n",
    "    n_samples = len(data) #total samples in data\n",
    "    L = np.zeros(n_nodes,dtype = int) #majority label\n",
    "    p1 = np.zeros(n_nodes) #empirical label frequency\n",
    "    n = np.zeros(n_nodes) #number of points sampled from each node\n",
    "    error = []#np.zeros(n_samples) #error at each round\n",
    "    root = n_nodes-1 #corresponds to index of root\n",
    "    P = np.array([root])\n",
    "    L[root] = 1    \n",
    "\n",
    "    for i in range(budget):\n",
    "        selected_P = []\n",
    "        for b in range(batch_size):\n",
    "            #TODO: select a node from P biasing towards choosing nodes in areas where the observed labels are less pure\n",
    "            raise(NotImplemetedError)\n",
    "            \n",
    "            #TODO: pick a random leaf node from subtree Tv and query its label\n",
    "\n",
    "            #TODO: update empirical counts and probabilities for all nodes u on path from z to v\n",
    "\n",
    "        for p in selected_P:\n",
    "            #TODO: update admissible A and compute scores; find best pruning and labeling\n",
    "            raise(NotImplemetedError)\n",
    "            #TODO: update pruning P and labeling L\n",
    "\n",
    "        #TODO: temporarily assign labels to every leaf and compute error\n",
    "        L_temp = L.copy()\n",
    "        raise(NotImplemetedError)\n",
    "        \n",
    "    for i in range(len(P)):\n",
    "        L = assign_labels(L,P[i],P[i],T,n_samples)\n",
    "        \n",
    "    return L, np.array(error)                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-generation",
   "metadata": {},
   "source": [
    "## 2.2 Run the sample code (10 points)\n",
    "Run the following sample code and compare the two figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-shark",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_DH(part,clf,budget):\n",
    "    \"\"\"Main function to run all your code once complete.  After you complete\n",
    "       select_case_1() and select_case_2(), this will run the DH algo for each \n",
    "       dataset and generate the plots you will submit within your write-up.\n",
    "\n",
    "       :param part: which part of the homework to run\n",
    "       :param clf: The classifier used to predcit on the dataset.\n",
    "       :param budget: The number of times that one can query a label from the oracle.\n",
    "    \"\"\"\n",
    "    \n",
    "    part = part.lower()\n",
    "    num_trials = 5\n",
    "    batch_size = 10\n",
    "    clf2 = copy.deepcopy(clf)\n",
    "    axs = plt.subplot()\n",
    "    if part == \"b\":\n",
    "        print(\"Running part B\")\n",
    "        X_train, y_train, X_test, y_test, T = load_data()\n",
    "        l = np.zeros(budget)\n",
    "        for i in range(num_trials):\n",
    "            print(\"Currently on iteration {}\".format(i))\n",
    "            L, error = select_case_1(X_train,y_train,T,budget,batch_size)\n",
    "            l += error \n",
    "        l /= num_trials\n",
    "        \n",
    "        ## TODO: train the classifier clf on the predicted label.\n",
    "        #raise(NotImplementedError)\n",
    "        clf.fit(X_train,L[:len(X_train)])\n",
    "        \n",
    "        print('Accuracy of classifier trained on random sampling dataset: \\t{:.3f}'.format(clf.score(X_test,y_test)))\n",
    "        axs.plot(np.arange(budget),l,label = \"Random sampling\")\n",
    "\n",
    "    elif part == \"c\":\n",
    "        print(\"Running part C\")\n",
    "        X_train, y_train, X_test, y_test, T = load_data()\n",
    "        l = np.zeros(budget)\n",
    "        for i in range(num_trials):\n",
    "            print(\"Currently on iteration {}\".format(i))\n",
    "            L, error = select_case_2(X_train,y_train,T,budget,batch_size)\n",
    "            l += error \n",
    "        l /= num_trials\n",
    "        \n",
    "        ## TODO: train the classifier clf2 on the predicted label.\n",
    "        #raise(NotImplementedError)\n",
    "        clf2.fit(X_train,L[:len(X_train)])\n",
    "        \n",
    "        print('Accuracy of classifier trained on active learning dataset: \\t{:.3f}'.format(clf2.score(X_test,y_test)))\n",
    "        axs.plot(np.arange(budget),l,label = \"Active learning\")\n",
    "\n",
    "    else:\n",
    "        print(\"Incorrect part provided. Either 'b', 'c', 'd', or 'e' expected\")\n",
    "    axs.set_ylim([0,0.5])\n",
    "    axs.set_xlabel(\"Number of query samples\")\n",
    "    axs.set_ylabel(\"Error rate\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"q2_2.png\")\n",
    "BUDGET = 200 #You can change this number to a smaller one during testing.\n",
    "for part in \"bc\":\n",
    "    call_DH(part,classifier,BUDGET)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-symbol",
   "metadata": {},
   "source": [
    "## 2.3 Questions (10 points):\n",
    "### What is a \"admissible pair\" according to the paper (5 points)?\n",
    "### Please explain the sampling bias that is dealt with in the DH algorithm and why it would be a problem if we just query the unlabeled point which is closest to the decision boundary (5 points)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
