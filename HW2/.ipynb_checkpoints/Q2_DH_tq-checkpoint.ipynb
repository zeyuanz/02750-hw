{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: DH algorithm (50 points)\n",
    "In this question we are going to implmeneted the DH algorithm according to this paper:https://icml.cc/Conferences/2008/papers/324.pdf, in which we try to predict protein localization sites in Eukaryotic cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import warnings\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import choice\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "seed = 2021\n",
    "warnings. filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Data loading and hierarchical clustering\n",
    "DH algorithm is based on hierarchical clustering of the dataset, we will use the DH algorithm on this classification problem: [Protein Localization Prediction](https://archive.ics.uci.edu/ml/datasets/Yeast), the first step is to load the dataset and conduct a hierarchical clustring using the **Scipy** package. *This part has been implemented, read through the code to make sure you understand what is being done.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(seed = 2021):\n",
    "    \"\"\" Loads \"Protein Localizataion Prediction\" data. Computes linkage from hierarchical clustering.\n",
    "    :returns X_train: data matrix 538x8\n",
    "    :returns Y_train: true labels 538x1\n",
    "    :returns X_test: data matrix 135x8\n",
    "    :returns Y_test: true labels 135x1\n",
    "    :returns T: 3 element tree\n",
    "        T[0] = linkage matrix from hierarchical clustering.  See https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html\n",
    "               for details. If you are unfamiliar with hierarchical clustering using scipy, the following is another helpful resource (We won't use dendrograms\n",
    "               here, but he gives a nice explanation of how to interpret the linkage matrix):\n",
    "               https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/ \n",
    "\n",
    "        T[1] = An array denoting the size of each subtree rooted at node i, where i indexes the array.  \n",
    "               ie. The number of all leaves in subtree rooted at node i (w_i in the paper).\n",
    "\n",
    "        T[2] = dict where keys are nodes and values are the node's parent\n",
    "        \"\"\"\n",
    "    df = pd.read_csv('./data/data.csv')\n",
    "    np.unique(df.Label,return_counts = True)\n",
    "    filter_class = ['MIT','NUC']\n",
    "    mask = df.Label ==0\n",
    "    for x in filter_class:\n",
    "        mask = mask | (df.Label==x)\n",
    "    df = df[mask]\n",
    "    X = df.iloc[:,:8].to_numpy()\n",
    "    y = df.Label.astype('category').cat.codes.to_numpy()\n",
    "    X, X_test, y, y_test = train_test_split(X,y,test_size = 0.2, random_state = seed)\n",
    "    n_samples = len(X)\n",
    "    Z = linkage(X,method='ward')\n",
    "    link = Z[:,:2].astype(int)\n",
    "    subtree_sizes = np.zeros(link[-1,-1]+2)\n",
    "    subtree_sizes[:n_samples] = 1\n",
    "    parent = {}\n",
    "    parent[2*(n_samples-1)] = -1 #set root node as 0\n",
    "    for i in range(len(link)):\n",
    "        left = link[i,0]\n",
    "        right = link[i,1]\n",
    "        current = i + n_samples\n",
    "        subtree_sizes[current] = subtree_sizes[left] + subtree_sizes[right] \n",
    "        parent[left] = current\n",
    "        parent[right] = current\n",
    "\n",
    "    T = [link,subtree_sizes,parent]\n",
    "\n",
    "    return X.astype(\"float\"), y, X_test, y_test, T \n",
    "\n",
    "X_train, y_train, X_test, y_test, T = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "## Random Forest\n",
    "N_estimator_rf = 20\n",
    "MAX_depth_rf = 6\n",
    "rf = RandomForestClassifier(n_estimators = N_estimator_rf, \n",
    "                            max_depth = MAX_depth_rf, random_state = seed)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "## Gradient Boosting Decision Tree\n",
    "N_estimator_gbdt = 20\n",
    "gbdt_max_depth = 6\n",
    "gbdt = GradientBoostingClassifier(n_estimators = N_estimator_gbdt,\n",
    "                                 learning_rate = 0.1,\n",
    "                                 max_depth = gbdt_max_depth,\n",
    "                                 random_state = seed)\n",
    "gbdt.fit(X_train,y_train)\n",
    "\n",
    "## 3-Layer fully connected NN\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "torch.manual_seed(seed)\n",
    "class NNClassifier(object):\n",
    "    def __init__(self,\n",
    "                 feature_n,\n",
    "                 class_n,\n",
    "                 hidden_n = 30,\n",
    "                 learning_rate = 4e-3,\n",
    "                 weight_decay = 1e-5):\n",
    "        self.model = torch.nn.Sequential(torch.nn.Linear(feature_n,hidden_n),\n",
    "                                        torch.nn.SiLU(),\n",
    "                                        torch.nn.Linear(hidden_n,hidden_n),\n",
    "                                        torch.nn.SiLU(),\n",
    "                                        torch.nn.Linear(hidden_n,class_n))\n",
    "        self.model =self.model.cuda()\n",
    "        self.lr = learning_rate\n",
    "        self.wd = weight_decay\n",
    "    def fit(self,X_train,y_train,epoches = 300,batch_size = 50):\n",
    "        X_t = torch.from_numpy(X_train.astype(np.float32))\n",
    "        y_t = torch.from_numpy(y_train.astype(np.int64))\n",
    "        dataset = TensorDataset(X_t,y_t)\n",
    "        loader = DataLoader(dataset,batch_size = batch_size,shuffle = True)\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(reduction = 'mean').cuda()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr,weight_decay=self.wd)\n",
    "        loss_record = 0.0\n",
    "        report_epoch = 50\n",
    "        for epoch_i in range(epoches):\n",
    "            for batch in loader:\n",
    "                x_batch,y_batch = batch\n",
    "                x_batch = x_batch.cuda()\n",
    "                y_batch = y_batch.cuda()\n",
    "                y_pred = self.model(x_batch)\n",
    "                loss = loss_fn(y_pred,y_batch)\n",
    "                self.model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_record += loss.item()\n",
    "            if epoch_i%report_epoch == report_epoch-1:\n",
    "                print(\"[%d|%d] epoch loss:%.2f\"%(epoch_i+1,epoches,loss_record/report_epoch))\n",
    "                loss_record = 0.0\n",
    "            if epoch_i>=epoches:\n",
    "                break\n",
    "    \n",
    "    def score(self,X_test,y_test):\n",
    "        X_test_tensor = torch.from_numpy(X_test.astype(np.float32))\n",
    "        y_pred_test = self.model(X_test_tensor)\n",
    "        y_output = torch.argmax(y_pred_test,axis = 1).numpy()\n",
    "        return (y_output == y_test).mean()\n",
    "        \n",
    "nn = NNClassifier(feature_n = X_train.shape[1],class_n = len(np.unique(y_train)))\n",
    "nn.fit(X_train,y_train)\n",
    "\n",
    "## Accuracy of 4 classifiers.\n",
    "print('Accuracy of logistic regression: \\t{:.3f}'.format(lr.score(X_test,y_test)))\n",
    "print('Accuracy of random forest: \\t\\t{:.3f}'.format(rf.score(X_test,y_test)))\n",
    "print('Accuracy of Gradient Boosting Decision Tree: \\t\\t{:.3f}'.format(gbdt.score(X_test,y_test)))\n",
    "print('Accuracy of Neural Network: \\t\\t{:.3f}'.format(nn.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0.1 Supervised classification methods.\n",
    "Following we provide several classifiers that can be used, choose your favourite one. To use the Neural Network classifier, you need to install [pytorch](https://pytorch.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose and initialize your classifier:\n",
    "The classifier is going to be used in 2.2, the choose of classifier won't influence your grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Uncomment one line to choose your classifier.\n",
    "#classifier = LogisticRegression()\n",
    "\n",
    "#classifier = RandomForestClassifier(n_estimators = N_estimator_rf,max_depth = MAX_depth_rf, random_state = seed)\n",
    "\n",
    "#classifier = GradientBoostingClassifier(n_estimators = N_estimator_gbdt,\n",
    "#                                 learning_rate = 0.1,\n",
    "#                                 max_depth = gbdt_max_depth,\n",
    "#                                 random_state = seed)\n",
    "\n",
    "classifier = NNClassifier(feature_n = X_train.shape[1],class_n = len(np.unique(y_train)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Implement the DH algorithm (Hierarchical Sampling for Active Learning). (30 points)\n",
    "Please complete the functions to implement the DH algorithm and run the active learning algorithm on the training dataset. The utils functions has been implemented and attached in the homework folder, including **update_empirical.py, best_pruning_and_labeling.py, assign_labels.py and get_leaves.py**, please read them and finish the following functions to implement the DH algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from update_empirical import update_empirical\n",
    "from best_pruning_and_labeling import best_pruning_and_labeling\n",
    "from assign_labels import assign_labels\n",
    "from get_leaves import get_leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(L,labels):\n",
    "    \"\"\"Compute the error\n",
    "\n",
    "    :param L: labeling of leaf nodes\n",
    "    :param labels: true labels of each node\n",
    "\n",
    "    :returns error: error of predictions\"\"\"\n",
    "\n",
    "    wrong = 0\n",
    "    wrong = (L[:len(labels)]!=labels).sum()\n",
    "    error = wrong/len(labels)\n",
    "    return error\n",
    "\n",
    "def select_case_1(data,labels,T,budget,batch_size):\n",
    "    \"\"\"DH algorithm where we choose P proportional to the size of subtree rooted at each node\n",
    "\n",
    "    :param data: Data matrix 1200x8\n",
    "    :param labels: true labels 1200x1\n",
    "    :param T: 3 element tree\n",
    "        T[0] = linkage matrix from hierarchical clustering.  See https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html\n",
    "               for details. If you are unfamiliar with hierarchical clustering using scipy, the following is another helpful resource (We won't use dendrograms\n",
    "               here, but he gives a nice explanation of how to interpret the linkage matrix):\n",
    "               https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/ \n",
    "\n",
    "        T[1] = An array denoting the size of each subtree rooted at node i, where i indexes the array.  \n",
    "               ie. The number of all children + grandchildren + ... + the node itself\n",
    "\n",
    "        T[2] = dict where keys are nodes and values are the node's parent\n",
    "    :param budget: Number of iterations to make \n",
    "    :param batch_size: Number of queries per iteration\"\"\"\n",
    "\n",
    "    n_nodes = len(T[1]) #total nodes in T\n",
    "    n_samples = len(data) #total samples in data\n",
    "    L = np.zeros(n_nodes) #majority label\n",
    "    p1 = np.zeros(n_nodes) #empirical label frequency (for label 1)\n",
    "    n = np.zeros(n_nodes) #number of points sampled from each node\n",
    "    error = []#np.zeros(n_samples) #error at each round\n",
    "    root = n_nodes-1 #corresponds to index of root\n",
    "    P = np.array([root])\n",
    "    L[root] = 1   \n",
    "\n",
    "    for i in range(budget):\n",
    "        selected_P = []\n",
    "        for b in range(batch_size):\n",
    "\n",
    "            #TODO: select a node from P proportional to the size of subtree rooted at each node\n",
    "            size_p = np.array([len(get_leaves([],v,T,n_samples)) / n_samples for v in P])\n",
    "            probs = size_p / sum(size_p)\n",
    "            select_node_index = np.random.choice(np.arange(len(P)), p=probs)\n",
    "            select_node = P[select_node_index]\n",
    "            selected_P.append(select_node)\n",
    "\n",
    "            ##TODO: pick a random leaf node from subtree Tv and query its label\n",
    "            leafs = get_leaves([],select_node,T,n_samples)\n",
    "            z = np.random.choice(leafs)\n",
    "            z_l = labels[z]\n",
    "\n",
    "            #TODO: update empirical counts and probabilities for all nodes u on path from z to v\n",
    "            n, p1 = update_empirical(n,p1,select_node,z,z_l,T)\n",
    "\n",
    "        for p in selected_P:\n",
    "            #TODO: update admissible A and compute scores; find best pruning and labeling\n",
    "            P_best, L_best = best_pruning_and_labeling(n,p1,p,T,n_samples)\n",
    "            #TODO: update pruning P and labeling L\n",
    "            P = np.array([node_ for node_ in P if node_ != p] + [node_ for node_ in P_best])\n",
    "            L[p] = L_best\n",
    "            \n",
    "            \n",
    "        #TODO: temporarily assign labels to every leaf and compute error\n",
    "        L_temp = L.copy()\n",
    "        for i in range(len(P)):\n",
    "            L_temp = assign_labels(L_temp,P[i],P[i],T,n_samples)\n",
    "        error_i = compute_error(L[:n_samples],labels)\n",
    "        error.append(error_i)\n",
    "\n",
    "    for i in range(len(P)):\n",
    "        L = assign_labels(L,P[i],P[i],T,n_samples)\n",
    "    \n",
    "    return L, np.array(error)\n",
    "\n",
    "def select_case_2(data,labels,T,budget,batch_size):\n",
    "    \"\"\"DH algorithm where we choose P by biasing towards choosing nodes in areas where the observed labels are less pure\n",
    "\n",
    "    :param data: Data matrix 1200x8\n",
    "    :param labels: true labels 284x1\n",
    "    :param T: 3 element tree\n",
    "        T[0] = linkage matrix from hierarchical clustering.  See https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html\n",
    "               for details. If you are unfamiliar with hierarchical clustering using scipy, the following is another helpful resource (We won't use dendrograms\n",
    "               here, but he gives a nice explanation of how to interpret the linkage matrix):\n",
    "               https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/ \n",
    "\n",
    "        T[1] = An array denoting the size of each subtree rooted at node i, where i indexes the array.  \n",
    "               ie. The number of all children + grandchildren + ... + the node itself\n",
    "\n",
    "        T[2] = dict where keys are nodes and values are the node's parent\n",
    "    :param budget: Number of iterations to make \n",
    "    :param batch_size: Number of queries per iteration\"\"\"\n",
    "\n",
    "    n_nodes = len(T[1]) #total nodes in T\n",
    "    n_samples = len(data) #total samples in data\n",
    "    L = np.zeros(n_nodes,dtype = int) #majority label\n",
    "    p1 = np.zeros(n_nodes) #empirical label frequency\n",
    "    n = np.zeros(n_nodes) #number of points sampled from each node\n",
    "    error = []#np.zeros(n_samples) #error at each round\n",
    "    root = n_nodes-1 #corresponds to index of root\n",
    "    P = np.array([root])\n",
    "    L[root] = 1    \n",
    "\n",
    "    for i in range(budget):\n",
    "        selected_P = []\n",
    "        for b in range(batch_size):\n",
    "            #TODO: select a node from P biasing towards choosing nodes in areas where the observed labels are less pure\n",
    "            size_p = np.array([len(get_leaves([],v,T,n_samples)) / n_samples for v in P])\n",
    "            weights = size_p / sum(size_p)\n",
    "            probs = weights\n",
    "            for idx, prune in enumerate(P):\n",
    "                lv = L[prune]\n",
    "                if lv == 1:\n",
    "                    pvlv = p1[prune]\n",
    "                else:\n",
    "                    pvlv = 1 - p1[prune]\n",
    "                upper_pvlv = min(1, pvlv - (1 / (1e-12 + n[prune]) + \\\n",
    "                                            np.sqrt((pvlv * (1 - pvlv)) / (1e-12 + n[prune])))\n",
    "                                )\n",
    "\n",
    "                probs[idx] = weights[idx] * (1 - upper_pvlv)\n",
    "            \n",
    "            if probs.sum() != 0:\n",
    "                probs = probs / probs.sum()\n",
    "            else:\n",
    "                probs = None\n",
    "            select_node_index = np.random.choice(np.arange(len(P)), p=probs)\n",
    "            select_node = P[select_node_index]\n",
    "            selected_P.append(select_node)\n",
    "            \n",
    "            #TODO: pick a random leaf node from subtree Tv and query its label\n",
    "            leafs = get_leaves([],select_node,T,n_samples)\n",
    "            z = np.random.choice(leafs)\n",
    "            z_l = labels[z]\n",
    "            #TODO: update empirical counts and probabilities for all nodes u on path from z to v\n",
    "            n, p1 = update_empirical(n,p1,select_node,z,z_l,T)\n",
    "            \n",
    "        for p in selected_P:\n",
    "            #TODO: update admissible A and compute scores; find best pruning and labeling\n",
    "            P_best, L_best = best_pruning_and_labeling(n,p1,p,T,n_samples)\n",
    "            #TODO: update pruning P and labeling L\n",
    "            P = np.array([node_ for node_ in P if node_ != p] + [node_ for node_ in P_best])\n",
    "            L[p] = L_best\n",
    "\n",
    "        #TODO: temporarily assign labels to every leaf and compute error\n",
    "        L_temp = L.copy()\n",
    "        error_i = compute_error(L[:n_samples],labels)\n",
    "        error.append(error_i)\n",
    "        \n",
    "    for i in range(len(P)):\n",
    "        L = assign_labels(L,P[i],P[i],T,n_samples)\n",
    "        \n",
    "    return L, np.array(error)                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Run the sample code (10 points)\n",
    "Run the following sample code and compare the two figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running part B\n",
      "Currently on iteration 0\n",
      "Currently on iteration 1\n",
      "Currently on iteration 2\n",
      "Currently on iteration 3\n",
      "Currently on iteration 4\n",
      "[50|300] epoch loss:5.32\n",
      "[100|300] epoch loss:5.18\n",
      "[150|300] epoch loss:5.07\n",
      "[200|300] epoch loss:5.00\n",
      "[250|300] epoch loss:4.95\n",
      "[300|300] epoch loss:4.90\n",
      "Accuracy of classifier trained on random sampling dataset: \t0.844\n",
      "Running part C\n",
      "Currently on iteration 0\n",
      "Currently on iteration 1\n",
      "Currently on iteration 2\n",
      "Currently on iteration 3\n",
      "Currently on iteration 4\n",
      "[50|300] epoch loss:3.96\n",
      "[100|300] epoch loss:3.76\n",
      "[150|300] epoch loss:3.71\n",
      "[200|300] epoch loss:3.64\n",
      "[250|300] epoch loss:3.55\n",
      "[300|300] epoch loss:3.49\n",
      "Accuracy of classifier trained on active learning dataset: \t0.867\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def call_DH(part,clf,budget):\n",
    "    \"\"\"Main function to run all your code once complete.  After you complete\n",
    "       select_case_1() and select_case_2(), this will run the DH algo for each \n",
    "       dataset and generate the plots you will submit within your write-up.\n",
    "\n",
    "       :param part: which part of the homework to run\n",
    "       :param clf: The classifier used to predcit on the dataset.\n",
    "       :param budget: The number of times that one can query a label from the oracle.\n",
    "    \"\"\"\n",
    "    \n",
    "    part = part.lower()\n",
    "    num_trials = 5\n",
    "    batch_size = 10\n",
    "    clf2 = copy.deepcopy(clf)\n",
    "    axs = plt.subplot()\n",
    "    if part == \"b\":\n",
    "        print(\"Running part B\")\n",
    "        X_train, y_train, X_test, y_test, T = load_data()\n",
    "        l = np.zeros(budget)\n",
    "        for i in range(num_trials):\n",
    "            print(\"Currently on iteration {}\".format(i))\n",
    "            L, error = select_case_1(X_train,y_train,T,budget,batch_size)\n",
    "            l += error \n",
    "        l /= num_trials\n",
    "        \n",
    "        ## TODO: train the classifier clf on the predicted label.\n",
    "        #raise(NotImplementedError)\n",
    "        clf.fit(X_train,L[:len(X_train)])\n",
    "        \n",
    "        print('Accuracy of classifier trained on random sampling dataset: \\t{:.3f}'.format(clf.score(X_test,y_test)))\n",
    "        axs.plot(np.arange(budget),l,label = \"Random sampling\")\n",
    "\n",
    "    elif part == \"c\":\n",
    "        print(\"Running part C\")\n",
    "        X_train, y_train, X_test, y_test, T = load_data()\n",
    "        l = np.zeros(budget)\n",
    "        for i in range(num_trials):\n",
    "            print(\"Currently on iteration {}\".format(i))\n",
    "            L, error = select_case_2(X_train,y_train,T,budget,batch_size)\n",
    "            l += error \n",
    "        l /= num_trials\n",
    "        \n",
    "        ## TODO: train the classifier clf2 on the predicted label.\n",
    "        #raise(NotImplementedError)\n",
    "        clf2.fit(X_train,L[:len(X_train)])\n",
    "        \n",
    "        print('Accuracy of classifier trained on active learning dataset: \\t{:.3f}'.format(clf2.score(X_test,y_test)))\n",
    "        axs.plot(np.arange(budget),l,label = \"Active learning\")\n",
    "\n",
    "    else:\n",
    "        print(\"Incorrect part provided. Either 'b', 'c', 'd', or 'e' expected\")\n",
    "    axs.set_xlabel(\"Number of query samples\")\n",
    "    axs.set_ylabel(\"Error rate\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"q2_2_bc.png\")\n",
    "\n",
    "BUDGET = 200 #You can change this number to a smaller one during testing.\n",
    "for part in \"bc\":\n",
    "    call_DH(part,classifier,BUDGET)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Questions (10 points):\n",
    "### What is a \"admissible pair\" according to the paper (5 points)?\n",
    "### Please explain the sampling bias that is dealt with in the DH algorithm and why it would be a problem if we just query the unlabeled point which is closest to the decision boundary (5 points)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. \"Admissible pair\"\n",
    "$$A_{v, l}(t)=True \\iff (1-p_{v,l}^{LB}(t)) < \\beta min_{l'\\neq l}(1-p_{v,l'}^{UB}(t))$$\n",
    "\n",
    "On the left, means the max error if v is paired with l, and the right represents the min error if v pair with other labels l'. So we consider (v, l) to be admissible, i.e. we are confident abount assigning l to v when even the worst mistake we can make by assigning l to v is better than the best other assignment can produce. \n",
    "\n",
    "\n",
    "\n",
    "## 2. Sampling bias\n",
    "The initial random sample would be bias so methods that are based on the initial sample to partition the hypothesis space would be biased since some rare case could be overlooked. On the other hand, if we incorporate clusters in the data, and query based on the agreement and the size within each cluster, it would avoid this problem at a high probability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
